{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1f2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dda21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe46695d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc6751fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(digits.images))\n",
    "print(type(digits.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eceaceff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "283f9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d19c5bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(15, 15))\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    f.add_subplot(4, 4, i + 1)\n",
    "    plt.imshow(np.array(digits.images[i]), cmap='binary')\n",
    "    plt.title(digits.target[i])\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fbe800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(digits.target.shape)\n",
    "print(digits.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e04bc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64),\n",
       " array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = digits.target\n",
    "x = digits.images.reshape(len(digits.images), -1)\n",
    "x.shape, x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e825c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:1000]\n",
    "y_train = y[:1000]\n",
    "x_test = x[1000:]\n",
    "y_test = y[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75433bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b134d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.22958289\n",
      "Iteration 2, loss = 1.91207743\n",
      "Iteration 3, loss = 1.62507727\n",
      "Iteration 4, loss = 1.32649842\n",
      "Iteration 5, loss = 1.06100535\n",
      "Iteration 6, loss = 0.83995513\n",
      "Iteration 7, loss = 0.67806075\n",
      "Iteration 8, loss = 0.55175832\n",
      "Iteration 9, loss = 0.45840445\n",
      "Iteration 10, loss = 0.39149735\n",
      "Iteration 11, loss = 0.33676351\n",
      "Iteration 12, loss = 0.29059880\n",
      "Iteration 13, loss = 0.25437208\n",
      "Iteration 14, loss = 0.22838372\n",
      "Iteration 15, loss = 0.20200554\n",
      "Iteration 16, loss = 0.18186565\n",
      "Iteration 17, loss = 0.16461183\n",
      "Iteration 18, loss = 0.14990228\n",
      "Iteration 19, loss = 0.13892154\n",
      "Iteration 20, loss = 0.12833784\n",
      "Iteration 21, loss = 0.12138920\n",
      "Iteration 22, loss = 0.11407971\n",
      "Iteration 23, loss = 0.10677664\n",
      "Iteration 24, loss = 0.10037149\n",
      "Iteration 25, loss = 0.09593187\n",
      "Iteration 26, loss = 0.09250135\n",
      "Iteration 27, loss = 0.08676698\n",
      "Iteration 28, loss = 0.08356043\n",
      "Iteration 29, loss = 0.08209789\n",
      "Iteration 30, loss = 0.07649168\n",
      "Iteration 31, loss = 0.07410898\n",
      "Iteration 32, loss = 0.07126869\n",
      "Iteration 33, loss = 0.06926956\n",
      "Iteration 34, loss = 0.06578496\n",
      "Iteration 35, loss = 0.06374913\n",
      "Iteration 36, loss = 0.06175492\n",
      "Iteration 37, loss = 0.05975664\n",
      "Iteration 38, loss = 0.05764485\n",
      "Iteration 39, loss = 0.05623663\n",
      "Iteration 40, loss = 0.05420966\n",
      "Iteration 41, loss = 0.05413911\n",
      "Iteration 42, loss = 0.05256140\n",
      "Iteration 43, loss = 0.05020265\n",
      "Iteration 44, loss = 0.04902779\n",
      "Iteration 45, loss = 0.04788382\n",
      "Iteration 46, loss = 0.04655532\n",
      "Iteration 47, loss = 0.04586089\n",
      "Iteration 48, loss = 0.04451758\n",
      "Iteration 49, loss = 0.04341598\n",
      "Iteration 50, loss = 0.04238096\n",
      "Iteration 51, loss = 0.04162200\n",
      "Iteration 52, loss = 0.04076839\n",
      "Iteration 53, loss = 0.04003180\n",
      "Iteration 54, loss = 0.03907774\n",
      "Iteration 55, loss = 0.03815565\n",
      "Iteration 56, loss = 0.03791975\n",
      "Iteration 57, loss = 0.03706276\n",
      "Iteration 58, loss = 0.03617874\n",
      "Iteration 59, loss = 0.03593227\n",
      "Iteration 60, loss = 0.03504175\n",
      "Iteration 61, loss = 0.03441259\n",
      "Iteration 62, loss = 0.03397449\n",
      "Iteration 63, loss = 0.03326990\n",
      "Iteration 64, loss = 0.03305025\n",
      "Iteration 65, loss = 0.03244893\n",
      "Iteration 66, loss = 0.03191504\n",
      "Iteration 67, loss = 0.03132169\n",
      "Iteration 68, loss = 0.03079707\n",
      "Iteration 69, loss = 0.03044946\n",
      "Iteration 70, loss = 0.03005546\n",
      "Iteration 71, loss = 0.02960555\n",
      "Iteration 72, loss = 0.02912799\n",
      "Iteration 73, loss = 0.02859103\n",
      "Iteration 74, loss = 0.02825959\n",
      "Iteration 75, loss = 0.02788968\n",
      "Iteration 76, loss = 0.02748725\n",
      "Iteration 77, loss = 0.02721247\n",
      "Iteration 78, loss = 0.02686225\n",
      "Iteration 79, loss = 0.02635636\n",
      "Iteration 80, loss = 0.02607439\n",
      "Iteration 81, loss = 0.02577613\n",
      "Iteration 82, loss = 0.02553642\n",
      "Iteration 83, loss = 0.02518749\n",
      "Iteration 84, loss = 0.02484300\n",
      "Iteration 85, loss = 0.02455379\n",
      "Iteration 86, loss = 0.02432480\n",
      "Iteration 87, loss = 0.02398548\n",
      "Iteration 88, loss = 0.02376004\n",
      "Iteration 89, loss = 0.02341261\n",
      "Iteration 90, loss = 0.02318255\n",
      "Iteration 91, loss = 0.02296065\n",
      "Iteration 92, loss = 0.02274048\n",
      "Iteration 93, loss = 0.02241054\n",
      "Iteration 94, loss = 0.02208181\n",
      "Iteration 95, loss = 0.02190861\n",
      "Iteration 96, loss = 0.02174404\n",
      "Iteration 97, loss = 0.02156939\n",
      "Iteration 98, loss = 0.02119768\n",
      "Iteration 99, loss = 0.02101874\n",
      "Iteration 100, loss = 0.02078230\n",
      "Iteration 101, loss = 0.02061573\n",
      "Iteration 102, loss = 0.02039802\n",
      "Iteration 103, loss = 0.02017245\n",
      "Iteration 104, loss = 0.01997162\n",
      "Iteration 105, loss = 0.01989280\n",
      "Iteration 106, loss = 0.01963828\n",
      "Iteration 107, loss = 0.01941850\n",
      "Iteration 108, loss = 0.01933154\n",
      "Iteration 109, loss = 0.01911473\n",
      "Iteration 110, loss = 0.01905371\n",
      "Iteration 111, loss = 0.01876085\n",
      "Iteration 112, loss = 0.01860656\n",
      "Iteration 113, loss = 0.01848655\n",
      "Iteration 114, loss = 0.01834844\n",
      "Iteration 115, loss = 0.01818981\n",
      "Iteration 116, loss = 0.01798523\n",
      "Iteration 117, loss = 0.01783630\n",
      "Iteration 118, loss = 0.01771441\n",
      "Iteration 119, loss = 0.01749814\n",
      "Iteration 120, loss = 0.01738339\n",
      "Iteration 121, loss = 0.01726549\n",
      "Iteration 122, loss = 0.01709638\n",
      "Iteration 123, loss = 0.01698340\n",
      "Iteration 124, loss = 0.01684606\n",
      "Iteration 125, loss = 0.01667016\n",
      "Iteration 126, loss = 0.01654172\n",
      "Iteration 127, loss = 0.01641832\n",
      "Iteration 128, loss = 0.01630111\n",
      "Iteration 129, loss = 0.01623051\n",
      "Iteration 130, loss = 0.01612736\n",
      "Iteration 131, loss = 0.01590220\n",
      "Iteration 132, loss = 0.01582485\n",
      "Iteration 133, loss = 0.01571372\n",
      "Iteration 134, loss = 0.01560349\n",
      "Iteration 135, loss = 0.01557688\n",
      "Iteration 136, loss = 0.01534420\n",
      "Iteration 137, loss = 0.01527883\n",
      "Iteration 138, loss = 0.01517545\n",
      "Iteration 139, loss = 0.01503663\n",
      "Iteration 140, loss = 0.01501192\n",
      "Iteration 141, loss = 0.01482535\n",
      "Iteration 142, loss = 0.01471388\n",
      "Iteration 143, loss = 0.01463948\n",
      "Iteration 144, loss = 0.01454059\n",
      "Iteration 145, loss = 0.01441742\n",
      "Iteration 146, loss = 0.01431741\n",
      "Iteration 147, loss = 0.01428414\n",
      "Iteration 148, loss = 0.01416364\n",
      "Iteration 149, loss = 0.01406742\n",
      "Iteration 150, loss = 0.01402651\n",
      "Iteration 151, loss = 0.01389720\n",
      "Iteration 152, loss = 0.01381412\n",
      "Iteration 153, loss = 0.01371300\n",
      "Iteration 154, loss = 0.01362465\n",
      "Iteration 155, loss = 0.01357048\n",
      "Iteration 156, loss = 0.01348760\n",
      "Iteration 157, loss = 0.01339543\n",
      "Iteration 158, loss = 0.01331941\n",
      "Iteration 159, loss = 0.01320812\n",
      "Iteration 160, loss = 0.01315415\n",
      "Iteration 161, loss = 0.01308279\n",
      "Iteration 162, loss = 0.01302708\n",
      "Iteration 163, loss = 0.01290042\n",
      "Iteration 164, loss = 0.01289267\n",
      "Iteration 165, loss = 0.01277558\n",
      "Iteration 166, loss = 0.01277238\n",
      "Iteration 167, loss = 0.01261308\n",
      "Iteration 168, loss = 0.01260611\n",
      "Iteration 169, loss = 0.01248789\n",
      "Iteration 170, loss = 0.01239662\n",
      "Iteration 171, loss = 0.01231743\n",
      "Iteration 172, loss = 0.01227346\n",
      "Iteration 173, loss = 0.01223136\n",
      "Iteration 174, loss = 0.01217211\n",
      "Iteration 175, loss = 0.01208682\n",
      "Iteration 176, loss = 0.01204707\n",
      "Iteration 177, loss = 0.01200225\n",
      "Iteration 178, loss = 0.01188677\n",
      "Iteration 179, loss = 0.01184993\n",
      "Iteration 180, loss = 0.01175130\n",
      "Iteration 181, loss = 0.01171178\n",
      "Iteration 182, loss = 0.01166052\n",
      "Iteration 183, loss = 0.01163843\n",
      "Iteration 184, loss = 0.01154892\n",
      "Iteration 185, loss = 0.01147629\n",
      "Iteration 186, loss = 0.01142365\n",
      "Iteration 187, loss = 0.01136608\n",
      "Iteration 188, loss = 0.01128053\n",
      "Iteration 189, loss = 0.01128869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(15,),\n",
       "              learning_rate_init=0.1, random_state=1, solver='sgd',\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f62e9287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 3, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 5, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp.predict(x_test)\n",
    "predictions[:50] \n",
    "# we just look at the 1st 50 examples in the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c418389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 9, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:50] \n",
    "# true labels for the 1st 50 examples in the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "617d875e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9146800501882058"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7826ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
